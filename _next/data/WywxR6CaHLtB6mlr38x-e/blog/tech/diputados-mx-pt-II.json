{"pageProps":{"results":[{"id":6,"title":"Diputados MX. Part II","description":"Scrapping the ðŸ‡²ðŸ‡½ congress site using Python.","content":"The most impÃ²rtant part of this project was getting the data. The Congress Website has all kinds of interesting data but sometimes is kind of confusing. \n\nI proceeded to scrape the site using the `requests-html` library. This library has great flexibility and it's easy to learn. Here's the [**documentation**](https://docs.python-requests.org/projects/requests-html/en/latest/) is a library that helps with the scrapping process. \n\nBefore starting any kind of python project, we must create a python virtual environment. The virtual environment helps us to isolate our project from the rest of our files and assure reproducibility. \n\n```python\n#  Create the Virtual Environment using venv. \npython3 -m venv env\n\n# Activating the virtual environment\nsource env/bin/activate\n```\nOnce the Virtual Environment is created, we proceed to create our `requirements.txt` file\n\n```bash\ntouch requirements.txt\n```\nThis file contains all the libraries required for this project. \n\n```\n#requirements.txt\nipykernel\npandas\nrequests-html\n```\n\n\n\n","slug":"diputados-mx-pt-II","published_at":"2021-10-05T01:49:58.229Z","created_at":"2021-09-28T01:14:49.720Z","updated_at":"2021-10-05T01:49:58.242Z","user":{"id":1,"username":"gmarr","email":"me@gmarr.com","provider":"local","confirmed":true,"blocked":false,"role":1,"created_at":"2021-09-27T19:44:38.166Z","updated_at":"2021-09-27T19:46:35.202Z"}}],"source":{"compiledSource":"var c=Object.defineProperty,h=Object.defineProperties;var m=Object.getOwnPropertyDescriptors;var o=Object.getOwnPropertySymbols;var i=Object.prototype.hasOwnProperty,s=Object.prototype.propertyIsEnumerable;var p=(e,t,r)=>t in e?c(e,t,{enumerable:!0,configurable:!0,writable:!0,value:r}):e[t]=r,a=(e,t)=>{for(var r in t||(t={}))i.call(t,r)&&p(e,r,t[r]);if(o)for(var r of o(t))s.call(t,r)&&p(e,r,t[r]);return e},l=(e,t)=>h(e,m(t));var u=(e,t)=>{var r={};for(var n in e)i.call(e,n)&&t.indexOf(n)<0&&(r[n]=e[n]);if(e!=null&&o)for(var n of o(e))t.indexOf(n)<0&&s.call(e,n)&&(r[n]=e[n]);return r};const layoutProps={},MDXLayout=\"wrapper\";function MDXContent(r){var n=r,{components:e}=n,t=u(n,[\"components\"]);return mdx(MDXLayout,l(a(a({},layoutProps),t),{components:e,mdxType:\"MDXLayout\"}),mdx(\"p\",null,\"The most imp\\xF2rtant part of this project was getting the data. The Congress Website has all kinds of interesting data but sometimes is kind of confusing. \"),mdx(\"p\",null,\"I proceeded to scrape the site using the \",mdx(\"inlineCode\",{parentName:\"p\"},\"requests-html\"),\" library. This library has great flexibility and it's easy to learn. Here's the \",mdx(\"a\",a({parentName:\"p\"},{href:\"https://docs.python-requests.org/projects/requests-html/en/latest/\"}),mdx(\"strong\",{parentName:\"a\"},\"documentation\")),\" is a library that helps with the scrapping process. \"),mdx(\"p\",null,\"Before starting any kind of python project, we must create a python virtual environment. The virtual environment helps us to isolate our project from the rest of our files and assure reproducibility. \"),mdx(\"pre\",null,mdx(\"code\",a({parentName:\"pre\"},{className:\"language-python\"}),`#  Create the Virtual Environment using venv. \npython3 -m venv env\n\n# Activating the virtual environment\nsource env/bin/activate\n`)),mdx(\"p\",null,\"Once the Virtual Environment is created, we proceed to create our \",mdx(\"inlineCode\",{parentName:\"p\"},\"requirements.txt\"),\" file\"),mdx(\"pre\",null,mdx(\"code\",a({parentName:\"pre\"},{className:\"language-bash\"}),`touch requirements.txt\n`)),mdx(\"p\",null,\"This file contains all the libraries required for this project. \"),mdx(\"pre\",null,mdx(\"code\",a({parentName:\"pre\"},{}),`#requirements.txt\nipykernel\npandas\nrequests-html\n`)))}MDXContent.isMDXComponent=!0;\n","scope":{}}},"__N_SSG":true}